---
title: "Homework Assignment: Sentiment Analysis of Emma"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

**Title**: Exploring Joyful Language in Jane Austen’s *Emma* using Tidytext

### **Instructions**

Complete the following exercises using the appropriate packages in R. Ensure that your solutions are optimized and use functional programming principles where applicable.

1.  Load the necessary libraries.
2.  Answer each question in separate R code chunks.
3.  Provide detailed explanations for your approach.
4.  Submit the rendered HTML file.

#### **Objective**:

Use the **`tidytext`** package and three different **sentiment lexicons** (`nrc`, `afinn`, `bing`) to explore **positive/joyful words** in *Emma* by Jane Austen. You will tokenize the text, apply sentiment filters, visualize frequent sentiment words using `ggplot2`, and create a word cloud.

### **Tasks**:

1.  **Data Preparation**

    -   Load the `austen_books()` dataset from the **`janeaustenr`** package.
    -   Group by book and detect chapter boundaries using regex.
    -   Create `linenumber` and `chapter` columns.

2.  **Tokenization**

    -   Use `unnest_tokens()` to tokenize text into individual words.

3.  **Sentiment Analysis**

    -   Filter joy/positive words from **each** of the three sentiment lexicons:

        -   `nrc` (joy)
        -   `afinn` (positive scores ≥ 1)
        -   `bing` (positive)

    -   Join each with *Emma*'s text and:

        -   Count word frequency.
        -   Filter for frequently occurring words (`n > 50`).
        -   Visualize using a **bar chart** (`ggplot2`) and a **word cloud** (`wordcloud`).

4.  **Push to GitHub**

    -   Push your complete R script (`.R` file) to a **GitHub repository**.

    -   Your script should include:

        -   Data wrangling
        -   Sentiment filtering and joins
        -   Visualization code

    -   You must include **at least 5 meaningful commits** to document your workflow.

------------------------------------------------------------------------

## **Rubric 100 Points**

| Category                                       | Points  |
|------------------------------------------------|---------|
| Step 1: Data wrangling (linenumber, chapter)   | 10      |
| Step 2: Tokenization                           | 10      |
| Step 3a: NRC sentiment analysis + plots        | 10      |
| Step 3b: AFINN sentiment analysis + plots      | 10      |
| Step 3c: BING sentiment analysis + plots       | 10      |
| Code readability, structure, and comments      | 10      |
| Word cloud for each lexicon                    | 20      |
| GitHub submission                              |         |
| Commit history (at least 5 meaningful commits) | 20      |
| **Total**                                      | **100** |

Good Luck!

```{r}
# emma_sentiment_min.R
suppressPackageStartupMessages({
  library(dplyr); 
  library(tidyr); 
  library(stringr)
  library(janeaustenr)
})

set.seed(123)

# Data Prep
emma <- austen_books() |>
  filter(book == "Emma") |>
  group_by(book) |>
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, regex("^chapter\\s+[\\divxlc]+", ignore_case = TRUE))) + 1L
  ) |>
  ungroup()

```

```{r}
# TOKENIZATION
suppressPackageStartupMessages({
  library(tidytext)
})

data(stop_words)
emma_tokens <- emma |>
  tidytext::unnest_tokens(word, text) |>
  anti_join(stop_words, by = "word") |>
  filter(!stringr::str_detect(word, "^[0-9]+$"))
```

```{r}
# 3) SENTIMENT LEXICONS
suppressPackageStartupMessages(library(syuzhet))
vocab <- unique(emma_tokens$word)

nrc_mat  <- syuzhet::get_nrc_sentiment(vocab)
nrc_joy  <- tibble::tibble(word = vocab) |>
  dplyr::bind_cols(tibble::as_tibble(nrc_mat)) |>
  dplyr::filter(joy == 1L) |>
  dplyr::select(word)

afinn_pos <- tibble::tibble(word = vocab, val = syuzhet::get_sentiment(vocab, method = "afinn")) |>
  dplyr::filter(val > 0) |>
  dplyr::select(word)

bing_pos  <- tibble::tibble(word = vocab, val = syuzhet::get_sentiment(vocab, method = "bing")) |>
  dplyr::filter(val > 0) |>
  dplyr::select(word)

analyze <- function(tokens, lex_df, name, min_n = 50) {
  raw <- tokens |> inner_join(lex_df, by = "word") |> count(word, sort = TRUE)
  list(name = name, raw = raw, top = raw |> filter(n > min_n))
}

res <- list(
  analyze(emma_tokens, nrc_joy,  "NRC (joy)"),
  analyze(emma_tokens, afinn_pos,"AFINN (positive)"),
  analyze(emma_tokens, bing_pos, "BING (positive)")
)

```

```{r}
# 4) VISUALIZATIONS
suppressPackageStartupMessages({
  library(ggplot2); library(forcats); library(wordcloud); library(purrr)
})

dir.create("figs", showWarnings = FALSE)

plot_bars <- function(df, title_txt) {
  df |>
    mutate(word = forcats::fct_reorder(word, n)) |>
    ggplot(aes(word, n)) +
    geom_col() +
    coord_flip() +
    labs(x = NULL, y = "Frequency", title = title_txt) +
    theme_minimal(base_size = 12)
}

purrr::walk(res, ~{
  nm <- .x$name
  g <- plot_bars(.x$top, paste0("Emma — Frequent Positive/Joy Words (", nm, ")"))
  ggsave(filename = file.path("figs", paste0(gsub("[^A-Za-z]+","_", tolower(nm)), "_bar.png")),
         plot = g, width = 9, height = 6, dpi = 120)

  png(file.path("figs", paste0(gsub("[^A-Za-z]+","_", tolower(nm)), "_cloud.png")),
      width = 900, height = 600, res = 120)
  suppressWarnings(wordcloud(words = .x$raw$word, freq = .x$raw$n, max.words = 150))
  title(main = paste0("Word Cloud — ", nm), line = -1)
  dev.off()
})

```

```{r}
# 5) CONSOLE SUMMARIES (tables only)
suppressPackageStartupMessages({ library(tibble); library(tidyr) })

# metrics per lexicon
summary_metrics <- purrr::map_dfr(res, \(x) {
  tibble(
    lexicon       = x$name,
    total_matches = sum(x$raw$n),
    unique_words  = nrow(x$raw)
  )
})
print(summary_metrics)

# Top-N words per lexicon (stacked table)
top_n <- 15
top_table <- purrr::map_dfr(res, \(x) {
  x$raw |> dplyr::slice_max(n, n = top_n) |> dplyr::mutate(lexicon = x$name)
}) |>
  dplyr::relocate(lexicon) |>
  dplyr::arrange(lexicon, dplyr::desc(n))
print(top_table)

# Words shared across all three lexicons, with per-lexicon counts
shared_words <- purrr::reduce(purrr::map(res, \(x) dplyr::distinct(x$raw, word)),
                              dplyr::inner_join, by = "word")

shared_counts <- shared_words |>
  dplyr::left_join(res[[1]]$raw |> dplyr::rename(n_1 = n), by = "word") |>
  dplyr::left_join(res[[2]]$raw |> dplyr::rename(n_2 = n), by = "word") |>
  dplyr::left_join(res[[3]]$raw |> dplyr::rename(n_3 = n), by = "word") |>
  dplyr::mutate(total = dplyr::coalesce(n_1,0) + dplyr::coalesce(n_2,0) + dplyr::coalesce(n_3,0)) |>
  dplyr::arrange(dplyr::desc(total)) |>
  dplyr::rename(
    !!res[[1]]$name := n_1,
    !!res[[2]]$name := n_2,
    !!res[[3]]$name := n_3
  )
print(shared_counts)

```

```{r}
# Helpers
plot_bars <- function(df, title_txt) {
  df |>
    dplyr::mutate(word = forcats::fct_reorder(word, n)) |>
    ggplot2::ggplot(ggplot2::aes(word, n)) +
    ggplot2::geom_col() +
    ggplot2::coord_flip() +
    ggplot2::labs(x = NULL, y = "Frequency", title = title_txt) +
    ggplot2::theme_minimal(base_size = 12)
}

# Try n > 50 first; if empty, relax to n >= 25; if still empty, plot top 20.
choose_plot_df <- function(x) {
  df50 <- dplyr::filter(x$raw, n > 50)
  if (nrow(df50) > 0) return(list(df = df50, note = "filtered: n > 50"))
  df25 <- dplyr::filter(x$raw, n >= 25)
  if (nrow(df25) > 0) return(list(df = df25, note = "filtered: n >= 25"))
  list(df = dplyr::slice_max(x$raw, n, n = 20), note = "top 20 by frequency")
}

# ---- Bar charts ----
bars_info <- purrr::map(res, choose_plot_df)
for (i in seq_along(res)) {
  nm   <- res[[i]]$name
  info <- bars_info[[i]]
  print(
    plot_bars(info$df, paste0("Emma — Frequent Words (", nm, ", ", info$note, ")"))
  )
}

```
